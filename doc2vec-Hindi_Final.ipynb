{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim `Doc2Vec` Tutorial on the Hindi Wikipedia Dataset\n",
    "# This tutorial is based on the original Gensim Tutorial for Doc2Vec. <a href=\"https://github.com/RaRe-Technologies/gensim/blob/ca0dcaa1eca8b1764f6456adac5719309e0d8e6d/docs/notebooks/doc2vec-IMDB.ipynb\"> link </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will learn how to apply Doc2vec using gensim by recreating the results of <a href=\"https://arxiv.org/pdf/1405.4053.pdf\">Le and Mikolov 2014</a>. \n",
    "\n",
    "### Outline: \n",
    "* Download the Data.\n",
    "* Clean the Data.\n",
    "* Tokenize the Data.\n",
    "* Build the Vocab\n",
    "* Train the Doc2Vec Model to generate the Word Embedings\n",
    "* Explore the Word and Doc Embeddings\n",
    "* Most Similar Words and Docs\n",
    "\n",
    "### `Word2Vec`\n",
    "`Word2Vec` is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, `strong` and `powerful` would be close together and `strong` and `Paris` would be relatively far. There are two versions of this model based on skip-grams (SG) and continuous-bag-of-words (CBOW), both implemented by the gensim `Word2Vec` class.\n",
    "\n",
    "\n",
    "But, Word2Vec doesn't yet get us fixed-size vectors for longer texts.\n",
    "\n",
    "\n",
    "### Paragraph Vector, aka gensim `Doc2Vec`\n",
    "The straightforward approach of averaging each of a text's words' word-vectors creates a quick and crude document-vector that can often be useful. However, Le and Mikolov in 2014 introduced the <i>Paragraph Vector</i>, which usually outperforms such simple-averaging.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like vector, which contributes to all training predictions, and is updated like other word-vectors, but we will call it a doc-vector. Gensim's `Doc2Vec` class implements this algorithm. \n",
    "\n",
    "#### Paragraph Vector - Distributed Memory (PV-DM)\n",
    "This is the Paragraph Vector model analogous to Word2Vec CBOW. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based an average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "#### Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "This is the Paragraph Vector model analogous to Word2Vec SG. The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full document's doc-vector. (It is also common to combine this with skip-gram testing, using both the doc-vector and nearby word-vectors to predict a single target word, but only one at a time.)\n",
    "\n",
    "### Requirements\n",
    "The following python modules are dependencies for this tutorial:\n",
    "* testfixtures ( `pip install testfixtures` )\n",
    "* statsmodels ( `pip install statsmodels` )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the wikipedia Hindi Data if it is not already downloaded (47 MB). This will be our text data for this tutorial.   \n",
    "The data can be found here: https://www.dropbox.com/s/p8bx1k3rn0b964r/hindi-wiki-data.7z?dl=0\n",
    "\n",
    "This cell will only reattempt steps such as Cleaning the Data. Download the Data first and Extract it to The directory of this notebook before running this Cell for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, alldata-id.txt is available for next steps.\n",
      "Wall time: 810 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "\n",
    "dirname = 'hindi-wiki-data'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\.\\\",\\(\\)!\\?;:])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('hindi-wiki-data/alldata-id.txt'):\n",
    "    if not os.path.isdir(dirname):\n",
    "        print(\"archive directory not available, please download.\")\n",
    "    else:\n",
    "        print(\"archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train','test', 'valid']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline = \"\\n\".encode(\"utf-8\")\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"hindi-wiki-data/alldata-id.txt\"), \"alldata-id.txt unavailable\"\n",
    "print(\"Success, alldata-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601230 docs: \n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "SentimentDocument = namedtuple('Document', 'words tags')\n",
    "\n",
    "alldocs = []\n",
    "with smart_open('hindi-wiki-data/alldata-id.txt', 'rb', encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        alldocs.append(SentimentDocument(words, tags))\n",
    "\n",
    "print('%d docs: ' % (len(alldocs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Check of how the Document looks now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(words=['क्षेत्र', 'की', 'कमी', 'और', 'मंगल', 'का', 'अत्यंत', 'पतला', 'वायुमंडल', 'एक', 'चुनौती', 'है', ':', 'इस', 'ग्रह', 'के', 'पास', ',', 'अपनी', 'सतह', 'के', 'आरपार', 'मामूली', 'ताप', 'संचरण', ',', 'सौर', 'वायु', 'के', 'हमले', 'के', 'खिलाफ', 'कमजोर', 'अवरोधक', 'और', 'पानी', 'को', 'तरल', 'रूप', 'में', 'बनाए', 'रखने', 'के', 'लिए', 'अपर्याप्त', 'वायु', 'मंडलीय', 'दाब', 'है।', 'मंगल', 'भी', 'करीब', 'करीब', ',', 'या', 'शायद', 'पूरी', 'तरह', 'से', ',', 'भूवैज्ञानिक', 'रूप', 'से', 'मृत', 'है', ';', 'ज्वालामुखी', 'गतिविधि', 'के', 'अंत', 'ने', 'उपरी', 'तौर', 'पर', 'ग्रह', 'के', 'भीतर', 'और', 'सतह', 'के', 'बीच', 'में', 'रसायनों', 'और', 'खनिजों', 'के', 'पुनर्चक्रण', '(', 'रीसाइक्लिंग', ')', 'को', 'बंद', 'कर', 'दिया', 'है।'], tags=[34993])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)\n",
    "doc_list[50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Doc2Vec Training & Build Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We vary the following parameter choices:\n",
    "* 50-dimensional vectors, as the 400-d vectors of the paper take a lot of memory and, in our tests of this task, don't seem to offer much benefit\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* Added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* A `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary scanned & state initialized\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=2, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. Here, we concatenate the paragraph vectors obtained from each model with the help of a thin wrapper class included in a gensim test module. (Note that this a separate, later concatenation of output-vectors than the kind of input-window-concatenation enabled by the `dm_concat=1` mode above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that doc-vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "(On a 4-core 1.6Ghz Intel Core i5, these 20 passes training and evaluating 3 main models takes about 3 hours.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc2,t8)\n",
      "Wall time: 38min 36s\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)\n",
      "Wall time: 1h 19min 44s\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc2,t8)\n",
      "Wall time: 42min 25s\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 288084...\n",
      "[-0.00299281  0.06446436 -0.087025   -0.00240485 -0.01394253 -0.20658462\n",
      " -0.11662332  0.03342903  0.08690358  0.08469858 -0.10076504 -0.07633572\n",
      "  0.05571734 -0.1240387  -0.08273178 -0.0545119  -0.2659948  -0.09168988\n",
      "  0.11924595  0.28239778  0.21871158  0.08158211  0.01639976  0.12588635\n",
      "  0.09813615  0.08310021  0.285711   -0.15112688  0.04318526  0.1822056\n",
      "  0.10976867 -0.00807587 -0.1847904  -0.05084115 -0.0724686   0.1298497\n",
      " -0.26680115 -0.05710117  0.02228931 -0.37422612  0.09797944 -0.02640666\n",
      "  0.08247092 -0.17209062 -0.3326172  -0.02629808  0.4261826   0.1485871\n",
      "  0.22895584 -0.2637845   0.1261971   0.05094935 -0.2904289   0.13632144\n",
      " -0.16419807 -0.1945721  -0.23614773 -0.02044909  0.1173032   0.35706726\n",
      "  0.13505965 -0.10583614 -0.01803126 -0.1905576   0.37404984  0.07108288\n",
      " -0.09450726 -0.14552376 -0.35908136  0.14968678  0.09081653  0.00218842\n",
      " -0.14549948 -0.0023069  -0.01853857  0.1493949  -0.24878217  0.14314504\n",
      "  0.42944664  0.09150154 -0.21470235  0.15616557  0.25951797 -0.2789927\n",
      "  0.2925494   0.48424056 -0.11316109  0.16966057 -0.3473253   0.0553013\n",
      " -0.08653067  0.21579626  0.15225331  0.17127006  0.15337922 -0.34696013\n",
      " -0.15629566 -0.05725697  0.07539571  0.03970889]\n",
      "Doc2Vec(dbow,d100,n5,mc2,t8):\n",
      " [(325069, 0.7500830888748169)]\n",
      "[ 0.16514064  0.7766467   0.5021962  -0.27987063 -0.2982577   0.9373281\n",
      " -0.7093608  -0.63114214  0.17015426  0.10432163 -0.46457168 -0.19310156\n",
      "  0.53761345  0.13033393 -0.2124057   0.10149605 -0.328577    0.5433177\n",
      "  0.06991959  0.14649206  0.5436434   0.14906862  0.16536042  0.3705787\n",
      " -0.42930245 -0.41992596 -0.24090202 -0.46825236  0.08630157  0.36544615\n",
      " -0.32189333 -0.64831746  0.28115872  0.28054997  0.32629395  0.68223923\n",
      "  0.03974016  0.12495426  0.49355164 -0.04578401 -0.46639013  0.02045165\n",
      " -0.95094323 -0.49014655 -0.12567692 -0.3515676  -0.11139518 -1.1337239\n",
      " -0.40287474  0.07828426  1.1772823  -0.27579677 -0.12551706  0.1779393\n",
      " -0.02941296  0.5902108   0.02996459 -0.2810652   0.02623514  1.1374058\n",
      "  0.185492   -0.3366065  -0.18679595 -0.0227918   0.37062773  0.8282581\n",
      "  0.5425537  -0.43295878 -0.09541085 -0.79659694 -0.10835414 -0.03870343\n",
      " -0.3208665   0.63591814  0.49817953 -0.6790613   0.37630403  0.09306082\n",
      " -0.09847986 -0.05813224 -0.23004591 -0.09270647  0.18179117  0.12168337\n",
      " -0.25297114  0.61703545  0.25737724  0.9364255  -0.14751802  0.47627953\n",
      "  0.3461726  -0.07684661  0.7244263   0.20679232 -0.05848566 -0.27598\n",
      " -0.5299908   0.59353524  0.84475476 -0.30714032]\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8):\n",
      " [(453480, 0.5408028960227966)]\n",
      "[ 3.48205492e-02  1.81349255e-02 -5.65368645e-02  2.96184290e-02\n",
      " -1.07896691e-02 -2.25784443e-02 -3.79677280e-03 -1.34069026e-01\n",
      " -6.28313869e-02 -7.33084381e-02 -3.83547023e-02 -2.90866476e-03\n",
      " -4.61756513e-02  3.53880338e-02 -1.11372896e-01  6.13610409e-02\n",
      "  2.63302661e-02 -8.38533863e-02 -3.20892259e-02  6.47877082e-02\n",
      " -2.71657612e-02  9.56732687e-03 -1.55853461e-02 -4.43359911e-02\n",
      "  3.07562351e-02  6.83582388e-03  7.52058476e-02 -5.65317878e-03\n",
      " -7.03849122e-02  3.45548578e-02  4.79060002e-02 -1.90193858e-02\n",
      "  1.03015304e-01  5.21804765e-02  8.09075404e-03  3.58326733e-02\n",
      " -2.48152413e-03 -3.50799449e-02  2.39172224e-02  4.81817983e-02\n",
      "  4.11824556e-03 -3.43255401e-02  1.92549489e-02  1.96034983e-02\n",
      "  5.40019013e-02 -3.35054100e-02 -1.51332482e-04  4.24184427e-02\n",
      " -7.32996874e-03  5.77650145e-02  1.30930841e-02  3.12289242e-02\n",
      " -5.61339594e-02  8.97994936e-02 -6.96640685e-02  1.34235801e-04\n",
      " -1.34805888e-02 -6.14193045e-02  4.08034474e-02  2.97703911e-02\n",
      "  9.59076360e-03  5.39129600e-02 -2.46176664e-02 -1.00312725e-01\n",
      "  3.20588462e-02  4.13959734e-02 -2.06341818e-02 -3.96710299e-02\n",
      "  1.42587591e-02 -3.12667340e-02  6.90018833e-02 -3.15944944e-03\n",
      "  3.79052348e-02 -9.55799446e-02 -3.96297342e-04  4.16042320e-02\n",
      " -6.07682206e-02  1.09437257e-02 -6.57728389e-02  9.60580260e-02\n",
      "  4.47666124e-02 -6.63905963e-02 -2.23461520e-02 -1.49817228e-01\n",
      " -5.95079036e-03 -3.40809412e-02  2.35407776e-03 -3.06279268e-02\n",
      "  1.15559883e-01  1.85923576e-02 -1.11611426e-01 -2.54151574e-03\n",
      "  3.71213146e-02 -4.67095897e-02 -4.55328003e-02  5.57477809e-02\n",
      " -5.78363165e-02  1.79546159e-02  2.28325017e-02 -5.25180660e-02]\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8):\n",
      " [(236915, 0.7591603994369507)]\n"
     ]
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    #inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    inferred_docvec = model.infer_vector(['अमेरिका', 'ईरान', 'जवाब','ट्रंप', 'बताया', 'सूट-बूट', 'आतंकी'])\n",
    "    print(inferred_docvec)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Defaults for inference may benefit from tuning for each dataset or model parameters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object contains the paragraph vectors learned from the training data. There will be one such vector for each unique document tag supplied during training. They may be individually accessed using the tag as an indexed-access key. For example, if one of the training documents used a tag of ‘245220’:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model in simple_models:\n",
    "    #model.docvecs['245220']\n",
    "    #model.save('hindi-wiki-data_model_docvec.d2v')\n",
    "    \n",
    "for i in range(3):\n",
    "     simple_models[i].save('hindi-wiki-data_model'+str(i)+'_docvec.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (443028): «प्रौद्योगिकी संस्थान , ताड़ेपल्लीगुड़म आंध्र प्रदेश में स्थित राष्ट्रीय महत्व का संस्थान है। यह भारत के प्रशिद्ध राष्ट्रीय प्रौद्योगिकी संस्थानों में से एक है। इसे 'एनआईटी ताड़ेपल्लीगुड़म' या 'एनआईटी आंध्र प्रदेश' के नाम से भी जाना जाता है। इस संस्थान में शिक्षण कार्य सन २०१५ से प्रारम्भ हुआ था। पूर्णकालिक कैंपस का निर्माण ४०० एकड़ में शुरू हो चुका है। इसके लिए पैसा मानव संसाधन विकास मंत्रालय देता है।»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8):\n",
      "\n",
      "MOST (443034, 0.8336281776428223): «प्रोद्योगिकी संस्थान , गोवा गोवा में स्थित राष्ट्रीय महत्व का संस्थान है। यह भारत के प्रशिद्ध राष्ट्रीय प्रौद्योगिकी संस्थानों में से एक है। इसे 'एनआईटी गोवा' के नाम से भी जाना जाता है। इस संस्थान में शिक्षण कार्य सन २०१० से प्रारम्भ हुआ था। पूर्णकालिक कैंपस का निर्माण ३०० एकड़ में शुरू हो चुका है। इसके लिए पैसा मानव संसाधन विकास मंत्रालय देता है।»\n",
      "\n",
      "MEDIAN (429652, 0.34647613763809204): «2014 में , एक अंतरराष्ट्रीय महिला टी-20 प्रतियोगिता के गठन , इंडियन प्रीमियर लीग की फ्रेंचाइजी मॉडल के आसपास आधारित की घोषणा की थी। पूर्व ऑस्ट्रेलियाई क्रिकेटर लिसा स्थालेकर और ऑस्ट्रेलियाई व्यवसायी शॉन मार्टिन के नेतृत्व में यह प्रस्ताव किया गया था कि छह टीमों , सिंगापुर में स्थित , सभी निजी स्वामित्व में किया जाएगा , और खिलाड़ियों को मौसम के अनुसार $us40 , 000 से अधिक कमाई हैं।»\n",
      "\n",
      "LEAST (31248, -0.39899134635925293): «»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[1].docvecs.count)  # pick random doc, re-run cell for more examples #155234\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat, in terms of Vocab and topics of document etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST... especially if the MOST has a cosine-similarity > 0.5. Re-run the cell to try another random target document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'संचालन' (2235 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc2,t8)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc2,t8)</th></tr><tr><td>[('437–438', 0.42513883113861084),<br>\n",
       "('endured', 0.4212109446525574),<br>\n",
       "('इंडियारेलइन्फो', 0.41686201095581055),<br>\n",
       "('‘अब', 0.4159364700317383),<br>\n",
       "('vtp', 0.405132532119751),<br>\n",
       "('फ़्रागार्या', 0.39857217669487),<br>\n",
       "('होलिवुड', 0.3927476108074188),<br>\n",
       "('//mod', 0.3907133936882019),<br>\n",
       "('संगटन', 0.38695940375328064),<br>\n",
       "('अप्रिल', 0.3860411047935486),<br>\n",
       "(\"ओवल'\", 0.3858442008495331),<br>\n",
       "('निम्नबल', 0.3831627368927002),<br>\n",
       "('हालार', 0.38268977403640747),<br>\n",
       "('سنڌي\\u200e', 0.38066890835762024),<br>\n",
       "('डेफ़िनिशन', 0.380027174949646),<br>\n",
       "('सेरामपोर', 0.3793221116065979),<br>\n",
       "('rreq', 0.37876227498054504),<br>\n",
       "('फुफ्फुसावरणशोथ', 0.37512099742889404),<br>\n",
       "('भागा।', 0.3748989701271057),<br>\n",
       "('बंजरपन', 0.37415167689323425)]</td><td>[('निर्माण', 0.8500988483428955),<br>\n",
       "('अभ्यास', 0.8122480511665344),<br>\n",
       "('पालन', 0.8114192485809326),<br>\n",
       "('उत्पादन', 0.8039084076881409),<br>\n",
       "('विस्तार', 0.7956109046936035),<br>\n",
       "('उपयोग', 0.7877824306488037),<br>\n",
       "('आयोजन', 0.783574640750885),<br>\n",
       "('अनुसरण', 0.783011257648468),<br>\n",
       "('समर्थन', 0.7800379991531372),<br>\n",
       "('प्रचार-प्रसार', 0.7774072885513306),<br>\n",
       "('प्रसारण', 0.7745814323425293),<br>\n",
       "('प्रचार', 0.7743381857872009),<br>\n",
       "('प्रसार', 0.7741221785545349),<br>\n",
       "('गठन', 0.7739957571029663),<br>\n",
       "('अध्ययन', 0.773598313331604),<br>\n",
       "('विकास', 0.7735896110534668),<br>\n",
       "('सृजन', 0.7734421491622925),<br>\n",
       "('प्रतिनिधित्व', 0.7727755904197693),<br>\n",
       "('चयन', 0.7726584672927856),<br>\n",
       "('निर्धारण', 0.7721700668334961)]</td><td>[('क्रियान्वयन', 0.7410501837730408),<br>\n",
       "('परिचालन', 0.7080305814743042),<br>\n",
       "('अनुरक्षण', 0.6725230813026428),<br>\n",
       "('रखरखाव', 0.6673260927200317),<br>\n",
       "('प्रबंधन', 0.6670118570327759),<br>\n",
       "('वित्तपोषण', 0.651642918586731),<br>\n",
       "('पर्यवेक्षण', 0.6502020359039307),<br>\n",
       "('प्रबन्धन', 0.6321255564689636),<br>\n",
       "('निष्पादन', 0.6200909614562988),<br>\n",
       "('रख-रखाव', 0.614891767501831),<br>\n",
       "('नवीनीकरण', 0.6146354675292969),<br>\n",
       "('समन्\\u200dवयन', 0.6104453802108765),<br>\n",
       "('निकास', 0.606842041015625),<br>\n",
       "('प्रशासन', 0.6063936352729797),<br>\n",
       "('पुनरोद्धार', 0.6056331396102905),<br>\n",
       "('संपादन', 0.5962674617767334),<br>\n",
       "('मूल्\\u200dयांकन', 0.5926483869552612),<br>\n",
       "('कार्यान्वयन', 0.5885398387908936),<br>\n",
       "('निर्माण', 0.5872750282287598),<br>\n",
       "('निस्तारण', 0.5830670595169067)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'साम्यवाद'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors – they remain at their random initialized values – unless you ask with the `dbow_words=1` initialization parameter.\n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'यूएसएसआर' or 'रूस'). (All DM modes inherently involve word-vector training concurrent with doc-vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Words in Vector Space\n",
    "source: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    count = 0\n",
    "    for word in model.wv.vocab:\n",
    "        if count != 500:\n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "            count++;\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_models = simple_models[:]\n",
    "tsne_plot(word_models[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other APIS of Doc2Vec: https://radimrehurek.com/gensim/models/doc2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
